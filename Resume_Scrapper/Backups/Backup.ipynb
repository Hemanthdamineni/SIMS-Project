{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted sections of autoCV (1).pdf: ['BASIC_INFO', 'SUMMARY', 'SKILLS', 'PROJECTS', 'EDUCATION']\n",
      "✅ Extracted sections of autoCV (2).pdf: ['BASIC_INFO', 'SUMMARY', 'SKILLS', 'EXPERIENCE', 'PROJECTS', 'EDUCATION']\n",
      "✅ Extracted sections of autoCV (3).pdf: ['BASIC_INFO', 'SUMMARY', 'SKILLS', 'EXPERIENCE', 'PROJECTS', 'EDUCATION']\n",
      "✅ Extracted sections of autoCV (4).pdf: ['BASIC_INFO', 'SUMMARY', 'SKILLS', 'EXPERIENCE', 'PROJECTS', 'EDUCATION']\n",
      "✅ Extracted sections of me.pdf: ['BASIC_INFO']\n",
      "Ranked Resumes (Highest to Lowest):\n",
      "Rank 1: /Resume_Scrapper/Resumes/autoCV (3).pdf (Score: 337.45)\n",
      "Rank 2: /Resume_Scrapper/Resumes/autoCV (4).pdf (Score: 290.29)\n",
      "Rank 3: /Resume_Scrapper/Resumes/autoCV (2).pdf (Score: 155.29)\n",
      "Rank 4: /Resume_Scrapper/Resumes/autoCV (1).pdf (Score: 123.43)\n",
      "Rank 5: /Resume_Scrapper/Resumes/me.pdf (Score: 0.0)\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file using pdfplumber.\"\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
    "    return text\n",
    "\n",
    "def parse_resume_sections(resume_text, resume_name):\n",
    "    \"\"\"Parses different sections from a resume text.\"\"\"\n",
    "    section_headers = [\"SUMMARY\", \"CONTACT\", \"OBJECTIVE\", \"REFERENCES\", \"SKILLS\", \"EDUCATION\", \"EXPERIENCE\", \"PROJECTS\"]\n",
    "    normalized_text = re.sub(r'[\\r\\u2022\\u200b]', '', resume_text)  # Remove special characters\n",
    "    normalized_text = re.sub(r'-\\n', '', normalized_text)  # Fix line breaks\n",
    "    normalized_text = \"\\n\" + normalized_text + \"\\n\"  # Buffer for boundary matching\n",
    "    \n",
    "    sections = {}\n",
    "    section_positions = []\n",
    "    for header in section_headers:\n",
    "        pattern = re.compile(rf'\\n\\s*{re.escape(header)}[\\s:•\\-]*\\n+', re.IGNORECASE)\n",
    "        for match in pattern.finditer(normalized_text):\n",
    "            section_positions.append((match.start(), header))\n",
    "    \n",
    "    section_positions.sort()\n",
    "    prev_end, prev_header = 0, \"BASIC_INFO\"\n",
    "    \n",
    "    for start, header in section_positions:\n",
    "        sections[prev_header] = normalized_text[prev_end:start].strip()\n",
    "        prev_end = start\n",
    "        prev_header = header.upper()\n",
    "    \n",
    "    sections[prev_header] = normalized_text[prev_end:].strip()\n",
    "    \n",
    "    print(f\"✅ Extracted sections of {resume_name}: {list(sections.keys())}\")\n",
    "    return sections\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Loads a dataset from a CSV file.\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def match_keywords(section_text, dataset, column_name, metric_column):\n",
    "    \"\"\"Finds matching entries from a dataset in the given section text.\"\"\"\n",
    "    if not section_text:\n",
    "        return pd.DataFrame(columns=[metric_column])\n",
    "    section_text = section_text.lower()\n",
    "    return dataset[dataset[column_name].str.lower().apply(lambda x: x in section_text)][[metric_column]]\n",
    "\n",
    "def calculate_resume_score(company_ranks, skills_scores, university_rankings, has_work_experience):\n",
    "    # Use raw scores instead of normalized scores\n",
    "    company_score = company_ranks['Rank'].min() if not company_ranks.empty else 0\n",
    "    skills_score = skills_scores['Score'].sum() if not skills_scores.empty else 0\n",
    "    university_score = university_rankings['ranking'].min() if not university_rankings.empty else 0\n",
    "    \n",
    "    # Assign weights\n",
    "    company_weight = 2.0  # Adjust as needed\n",
    "    skills_weight = 1.5   # Skills might be more important\n",
    "    university_weight = 1.0  # Universities might be less important\n",
    "    \n",
    "    # Calculate weighted total score\n",
    "    total_score = (\n",
    "        (company_score * company_weight) +\n",
    "        (skills_score * skills_weight) +\n",
    "        (university_score * university_weight)\n",
    "    )\n",
    "    \n",
    "    # Apply penalty for resumes without work experience\n",
    "    if not has_work_experience:\n",
    "        total_score *= 0.8  # Reduce score by 20% if no work experience\n",
    "    \n",
    "    # Scale the final score to a range of 0 to 100\n",
    "    scaled_score = (total_score / 10)  # Adjust the divisor based on expected score range\n",
    "    return round(scaled_score, 2)\n",
    "\n",
    "def process_resume(pdf_path, company_df, skills_df, universities_df):\n",
    "    \"\"\"Processes a resume and returns its score.\"\"\"\n",
    "    resume_text = extract_text_from_pdf(pdf_path)\n",
    "    resume_sections = parse_resume_sections(resume_text, pdf_path.split(\"/\")[-1])\n",
    "    \n",
    "    company_ranks = match_keywords(resume_sections.get(\"EXPERIENCE\", \"\"), company_df, 'Name', 'Rank')\n",
    "    skills_scores = match_keywords(resume_sections.get(\"SKILLS\", \"\"), skills_df, 'Skill', 'Score')\n",
    "    university_rankings = match_keywords(resume_sections.get(\"EDUCATION\", \"\"), universities_df, 'University', 'ranking')\n",
    "    \n",
    "    has_experience = \"EXPERIENCE\" in resume_sections and bool(resume_sections[\"EXPERIENCE\"].strip())\n",
    "    \n",
    "    return calculate_resume_score(company_ranks, skills_scores, university_rankings, has_experience)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company_df = load_dataset('/workspaces/SIMS-Project/Resume_Scrapper/Datasets/Companies_Dataset.csv')\n",
    "    skills_df = load_dataset('/workspaces/SIMS-Project/Resume_Scrapper/Datasets/Skills_Dataset.csv')\n",
    "    universities_df = load_dataset('/workspaces/SIMS-Project/Resume_Scrapper/Datasets/Universities_Dataset.csv')\n",
    "    \n",
    "    resume_files = [\n",
    "        '/workspaces/SIMS-Project/Resume_Scrapper/Resumes/autoCV (1).pdf',\n",
    "        '/workspaces/SIMS-Project/Resume_Scrapper/Resumes/autoCV (2).pdf',\n",
    "        '/workspaces/SIMS-Project/Resume_Scrapper/Resumes/autoCV (3).pdf',\n",
    "        '/workspaces/SIMS-Project/Resume_Scrapper/Resumes/autoCV (4).pdf',\n",
    "        '/workspaces/SIMS-Project/Resume_Scrapper/Resumes/me.pdf'\n",
    "    ]\n",
    "    \n",
    "    resume_scores = [(file, process_resume(file, company_df, skills_df, universities_df)) for file in resume_files]\n",
    "    resume_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Ranked Resumes (Highest to Lowest):\")\n",
    "    for rank, (file, score) in enumerate(resume_scores, start=1):\n",
    "        print(f\"Rank {rank}: {file[24:]} (Score: {score})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted sections of autoCV (1).pdf: ['BASIC_INFO', 'SUMMARY', 'SKILLS', 'PROJECTS', 'EDUCATION']\n",
      "Found 5 files in https://github.com/Karthik0000007/Disease-Predictor\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Resume_Scrapper/Downloaded/code_files/-Disease_Predictor.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 125\u001b[0m\n\u001b[1;32m    115\u001b[0m universities_df \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspaces/SIMS-Project/Resume_Scrapper/Datasets/Universities_Dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    117\u001b[0m resume_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspaces/SIMS-Project/Resume_Scrapper/Resumes/autoCV (1).pdf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspaces/SIMS-Project/Resume_Scrapper/Resumes/autoCV (3).pdf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspaces/SIMS-Project/Resume_Scrapper/Resumes/me.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    123\u001b[0m ]\n\u001b[0;32m--> 125\u001b[0m resume_scores \u001b[38;5;241m=\u001b[39m [(file, \u001b[43mprocess_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompany_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskills_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniversities_df\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resume_files]\n\u001b[1;32m    126\u001b[0m resume_scores\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRanked Resumes (Highest to Lowest):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 108\u001b[0m, in \u001b[0;36mprocess_resume\u001b[0;34m(pdf_path, company_df, skills_df, universities_df)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m extracted_links:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m link:\n\u001b[0;32m--> 108\u001b[0m         \u001b[43mfile_downloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDownloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m calculate_resume_score(company_ranks, skills_scores, university_rankings, has_experience)\n",
      "File \u001b[0;32m/workspaces/SIMS-Project/Resume_Scrapper/File_downloader_from_github.py:34\u001b[0m, in \u001b[0;36mDownloader\u001b[0;34m(repo_link, base_path)\u001b[0m\n\u001b[1;32m     31\u001b[0m folder \u001b[38;5;241m=\u001b[39m raw_url\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     35\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(file_response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Resume_Scrapper/Downloaded/code_files/-Disease_Predictor.py'"
     ]
    }
   ],
   "source": [
    "# import pdfplumber\n",
    "import read_resume\n",
    "import File_downloader_from_github as file_downloader\n",
    "import re\n",
    "import pandas as pd\n",
    "# import pdfplumber\n",
    "# import pdfminer.high_level as pm\n",
    "\n",
    "# def extract_text_and_links_from_pdf(pdf_path):\n",
    "#     text = pm.extract_text(pdf_path)\n",
    "#     links = []\n",
    "\n",
    "#     github_link_pattern = r\"https://github\\.com/[a-zA-Z0-9-_]+/[a-zA-Z0-9-_]+\"\n",
    "#     links += re.findall(github_link_pattern, text)\n",
    "\n",
    "#     with pdfplumber.open(pdf_path) as pdf:\n",
    "#         for page in pdf.pages:\n",
    "#             hyperlinks = page.hyperlinks\n",
    "#             if hyperlinks:\n",
    "#                 for hyperlink in hyperlinks:\n",
    "#                     if 'uri' in hyperlink:\n",
    "#                         links.append(hyperlink['uri'])\n",
    "\n",
    "#     links = list(set(links))\n",
    "#     return text, links\n",
    "\n",
    "def parse_resume_sections(resume_text, resume_name):\n",
    "    \"\"\"Parses different sections from a resume text.\"\"\"\n",
    "    section_headers = [\"SUMMARY\", \"CONTACT\", \"OBJECTIVE\", \"REFERENCES\", \"SKILLS\", \"EDUCATION\", \"EXPERIENCE\", \"PROJECTS\"]\n",
    "    normalized_text = re.sub(r'[\\r\\u2022\\u200b]', '', resume_text)  # Remove special characters\n",
    "    normalized_text = re.sub(r'-\\n', '', normalized_text)  # Fix line breaks\n",
    "    normalized_text = \"\\n\" + normalized_text + \"\\n\"  # Buffer for boundary matching\n",
    "    \n",
    "    sections = {}\n",
    "    section_positions = []\n",
    "    for header in section_headers:\n",
    "        pattern = re.compile(rf'\\n\\s*{re.escape(header)}[\\s:•\\-]*\\n+', re.IGNORECASE)\n",
    "        for match in pattern.finditer(normalized_text):\n",
    "            section_positions.append((match.start(), header))\n",
    "    \n",
    "    section_positions.sort()\n",
    "    prev_end, prev_header = 0, \"BASIC_INFO\"\n",
    "    \n",
    "    for start, header in section_positions:\n",
    "        sections[prev_header] = normalized_text[prev_end:start].strip()\n",
    "        prev_end = start\n",
    "        prev_header = header.upper()\n",
    "    \n",
    "    sections[prev_header] = normalized_text[prev_end:].strip()\n",
    "    \n",
    "    print(f\"✅ Extracted sections of {resume_name}: {list(sections.keys())}\")\n",
    "    return sections\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Loads a dataset from a CSV file.\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def match_keywords(section_text, dataset, column_name, metric_column):\n",
    "    \"\"\"Finds matching entries from a dataset in the given section text.\"\"\"\n",
    "    if not section_text:\n",
    "        return pd.DataFrame(columns=[metric_column])\n",
    "    section_text = section_text.lower()\n",
    "    return dataset[dataset[column_name].str.lower().apply(lambda x: x in section_text)][[metric_column]]\n",
    "\n",
    "def calculate_resume_score(company_ranks, skills_scores, university_rankings, has_work_experience):\n",
    "    # Use raw scores instead of normalized scores\n",
    "    company_score = company_ranks['Rank'].min() if not company_ranks.empty else 0\n",
    "    skills_score = skills_scores['Score'].sum() if not skills_scores.empty else 0\n",
    "    university_score = university_rankings['ranking'].min() if not university_rankings.empty else 0\n",
    "    \n",
    "    # Assign weights\n",
    "    company_weight = 2.0  # Adjust as needed\n",
    "    skills_weight = 1.5   # Skills might be more important\n",
    "    university_weight = 1.0  # Universities might be less important\n",
    "    \n",
    "    # Calculate weighted total score\n",
    "    total_score = (\n",
    "        (company_score * company_weight) +\n",
    "        (skills_score * skills_weight) +\n",
    "        (university_score * university_weight)\n",
    "    )\n",
    "    \n",
    "    # Apply penalty for resumes without work experience\n",
    "    if not has_work_experience:\n",
    "        total_score *= 0.8  # Reduce score by 20% if no work experience\n",
    "    \n",
    "    # Scale the final score to a range of 0 to 100\n",
    "    scaled_score = (total_score / 10)  # Adjust the divisor based on expected score range\n",
    "    return round(scaled_score, 2)\n",
    "\n",
    "def process_resume(pdf_path, company_df, skills_df, universities_df):\n",
    "    \"\"\"Processes a resume and returns its score.\"\"\"\n",
    "    resume_text, extracted_links = read_resume.extract_text_and_links_from_pdf(pdf_path)\n",
    "    resume_sections = parse_resume_sections(resume_text, pdf_path.split(\"/\")[-1])\n",
    "    \n",
    "    company_ranks = match_keywords(resume_sections.get(\"EXPERIENCE\", \"\"), company_df, 'Name', 'Rank')\n",
    "    skills_scores = match_keywords(resume_sections.get(\"SKILLS\", \"\"), skills_df, 'Skill', 'Score')\n",
    "    university_rankings = match_keywords(resume_sections.get(\"EDUCATION\", \"\"), universities_df, 'University', 'ranking')\n",
    "    \n",
    "    has_experience = \"EXPERIENCE\" in resume_sections and bool(resume_sections[\"EXPERIENCE\"].strip())\n",
    "    \n",
    "    # print(\"Extracted Links:\")\n",
    "    # for link in extracted_links:\n",
    "    #     print(link)\n",
    "\n",
    "    for link in extracted_links:\n",
    "        if \"github\" in link:\n",
    "            file_downloader.Downloader(link)\n",
    "\n",
    "    return calculate_resume_score(company_ranks, skills_scores, university_rankings, has_experience)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company_df = load_dataset('/workspaces/SIMS-Project/Resume_Scrapper/Datasets/Companies_Dataset.csv')\n",
    "    skills_df = load_dataset('/workspaces/SIMS-Project/Resume_Scrapper/Datasets/Skills_Dataset.csv')\n",
    "    universities_df = load_dataset('/workspaces/SIMS-Project/Resume_Scrapper/Datasets/Universities_Dataset.csv')\n",
    "    \n",
    "    resume_files = [\n",
    "        '/workspaces/SIMS-Project/Resume_Scrapper/Resumes/autoCV (1).pdf',\n",
    "        '/workspaces/SIMS-Project/Resume_Scrapper/Resumes/autoCV (3).pdf',\n",
    "        '/workspaces/SIMS-Project/Resume_Scrapper/Resumes/autoCV (4).pdf',\n",
    "        '/workspaces/SIMS-Project/Resume_Scrapper/Resumes/Resume_2.pdf',\n",
    "        '/workspaces/SIMS-Project/Resume_Scrapper/Resumes/me.pdf',\n",
    "    ]\n",
    "    \n",
    "    resume_scores = [(file, process_resume(file, company_df, skills_df, universities_df)) for file in resume_files]\n",
    "    resume_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Ranked Resumes (Highest to Lowest):\")\n",
    "    for rank, (file, score) in enumerate(resume_scores, start=1):\n",
    "        print(f\"Rank {rank}: {file[24:]} (Score: {score})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:✅ Extracted sections: ['BASIC_INFO', 'SUMMARY', 'SKILLS', 'EXPERIENCE', 'PROJECTS', 'EDUCATION']\n",
      "ERROR:root:Error processing /workspaces/SIMS-Project/Resume_Scrapper/Resumes/autoCV (3).pdf: [Errno 2] No such file or directory: 'Resume_Scrapper/Datasets/Companies_Dataset.csv'\n",
      "INFO:root:✅ Extracted sections: ['BASIC_INFO', 'SUMMARY', 'EXPERIENCE', 'PROJECTS', 'EDUCATION']\n",
      "WARNING:root:Section 'Skills' not found in resume.\n",
      "ERROR:root:Error processing /workspaces/SIMS-Project/Resume_Scrapper/Resumes/resume_2 (1).pdf: [Errno 2] No such file or directory: 'Resume_Scrapper/Datasets/Companies_Dataset.csv'\n",
      "INFO:root:✅ Extracted sections: ['BASIC_INFO', 'SUMMARY', 'SKILLS', 'PROJECTS', 'EDUCATION']\n",
      "WARNING:root:Section 'Experience' not found in resume.\n",
      "ERROR:root:Error processing /workspaces/SIMS-Project/Resume_Scrapper/Resumes/Resume.pdf: [Errno 2] No such file or directory: 'Resume_Scrapper/Datasets/Companies_Dataset.csv'\n",
      "INFO:root:✅ Extracted sections: ['BASIC_INFO', 'EDUCATION', 'SKILLS']\n",
      "WARNING:root:Section 'Experience' not found in resume.\n",
      "ERROR:root:Error processing /workspaces/SIMS-Project/Resume_Scrapper/Resumes/Resume_2.pdf: [Errno 2] No such file or directory: 'Resume_Scrapper/Datasets/Companies_Dataset.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Resumes (Highest to Lowest):\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "    \"\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def parse_resume_sections(resume_text):\n",
    "    \"\"\"\n",
    "    Parses the resume text into sections based on common headers.\n",
    "    \"\"\"\n",
    "    section_headers = [\n",
    "        'Summary', 'Skills', 'Education', 'Experience', 'Projects'\n",
    "    ]\n",
    "    \n",
    "    # Preprocess text for consistent matching\n",
    "    normalized_text = re.sub(r'[\\r\\u2022\\u200b]', '', resume_text)  # Remove special chars\n",
    "    normalized_text = re.sub(r'-\\n', '', normalized_text)  # Handle hyphenated line breaks\n",
    "    normalized_text = \"\\n\" + normalized_text + \"\\n\"  # Add buffer for boundary matches\n",
    "    \n",
    "    # Find all section positions\n",
    "    section_positions = []\n",
    "    for header in section_headers:\n",
    "        pattern = re.compile(\n",
    "            rf'\\n\\s*{re.escape(header)}\\s*\\n',\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        for match in pattern.finditer(normalized_text):\n",
    "            section_positions.append((match.start(), header.upper()))\n",
    "    \n",
    "    # Sort sections by appearance order\n",
    "    section_positions.sort()\n",
    "    sorted_sections = [header for _, header in section_positions]\n",
    "    \n",
    "    # Add Basic Info as first section if no sections found\n",
    "    if not sorted_sections:\n",
    "        sorted_sections = [\"BASIC_INFO\"]\n",
    "    \n",
    "    # Extract content between sections\n",
    "    sections = {}\n",
    "    prev_end = 0\n",
    "    prev_header = \"BASIC_INFO\"\n",
    "    \n",
    "    for start, header in section_positions:\n",
    "        sections[prev_header] = normalized_text[prev_end:start].strip()\n",
    "        prev_end = start\n",
    "        prev_header = header\n",
    "    \n",
    "    # Add final section\n",
    "    sections[prev_header] = normalized_text[prev_end:].strip()\n",
    "    \n",
    "    logging.info(f\"✅ Extracted sections: {list(sections.keys())}\")\n",
    "    return sections\n",
    "\n",
    "def save_sections_to_files(sections):\n",
    "    \"\"\"\n",
    "    Saves each section's content to a separate text file.\n",
    "    \"\"\"\n",
    "    for section_name, section_content in sections.items():\n",
    "        with open(f'{section_name.lower().replace(\" \", \"_\")}.txt', 'w') as file:\n",
    "            file.write(section_content)\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Loads a dataset from a CSV file.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def compare_and_extract_metrics(section_file, dataset, column_name, metric_column):\n",
    "    \"\"\"\n",
    "    Compares section content with a dataset and extracts relevant metrics.\n",
    "    \"\"\"\n",
    "    with open(section_file, 'r') as file:\n",
    "        section_content = file.read()\n",
    "    \n",
    "    # Find matches where the column_name appears in the section content\n",
    "    matches = dataset[dataset[column_name].apply(lambda x: str(x).lower() in section_content.lower())]\n",
    "    return matches[[metric_column]]\n",
    "\n",
    "def calculate_resume_score(company_ranks, skills_scores, university_rankings, has_work_experience):\n",
    "    \"\"\"\n",
    "    Calculates a resume score based on extracted metrics.\n",
    "    \"\"\"\n",
    "    # Use raw scores instead of normalized scores\n",
    "    company_score = company_ranks['Rank'].min() if not company_ranks.empty else 0\n",
    "    skills_score = skills_scores['Score'].sum() if not skills_scores.empty else 0\n",
    "    university_score = university_rankings['ranking'].min() if not university_rankings.empty else 0\n",
    "    \n",
    "    # Assign weights\n",
    "    company_weight = 2.0  # Adjust as needed\n",
    "    skills_weight = 1.5   # Skills might be more important\n",
    "    university_weight = 1.0  # Universities might be less important\n",
    "    \n",
    "    # Calculate weighted total score\n",
    "    total_score = (\n",
    "        (company_score * company_weight) +\n",
    "        (skills_score * skills_weight) +\n",
    "        (university_score * university_weight)\n",
    "    )\n",
    "    \n",
    "    # Apply penalty for resumes without work experience\n",
    "    if not has_work_experience:\n",
    "        total_score *= 0.8  # Reduce score by 20% if no work experience\n",
    "    \n",
    "    # Scale the final score to a range of 0 to 100\n",
    "    scaled_score = (total_score / 10)  # Adjust the divisor based on expected score range\n",
    "    return round(scaled_score, 2)\n",
    "\n",
    "def process_resume(pdf_path):\n",
    "    \"\"\"\n",
    "    Processes a resume PDF and calculates its score.\n",
    "    \"\"\"\n",
    "    # Extract text from PDF\n",
    "    resume_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Parse resume sections\n",
    "    resume_sections = parse_resume_sections(resume_text)\n",
    "    \n",
    "    # Log missing sections\n",
    "    expected_sections = ['Experience', 'Skills', 'Education']\n",
    "    for section in expected_sections:\n",
    "        if section.upper() not in resume_sections:\n",
    "            logging.warning(f\"Section '{section}' not found in resume.\")\n",
    "    \n",
    "    # Save sections to files\n",
    "    save_sections_to_files(resume_sections)\n",
    "    \n",
    "    # Load datasets\n",
    "    company_df = load_dataset('Resume_Scrapper/Datasets/Companies_Dataset.csv')\n",
    "    skills_df = load_dataset('Resume_Scrapper/Datasets/Skills_Dataset.csv')\n",
    "    universities_df = load_dataset('Resume_Scrapper/Datasets/Universities_Dataset.csv')\n",
    "    \n",
    "    # Compare and extract metrics\n",
    "    company_ranks = compare_and_extract_metrics('experience.txt', company_df, 'Name', 'Rank')\n",
    "    skills_scores = compare_and_extract_metrics('skills.txt', skills_df, 'Skill', 'Score')\n",
    "    university_rankings = compare_and_extract_metrics('education.txt', universities_df, 'University', 'ranking')\n",
    "    \n",
    "    # Check if work experience exists\n",
    "    has_work_experience = 'EXPERIENCE' in resume_sections and bool(resume_sections['EXPERIENCE'])\n",
    "    \n",
    "    # Calculate resume score\n",
    "    resume_score = calculate_resume_score(company_ranks, skills_scores, university_rankings, has_work_experience)\n",
    "    \n",
    "    return resume_score\n",
    "\n",
    "# List of resume files\n",
    "resume_files = [\n",
    "    '/workspaces/SIMS-Project/Resume_Scrapper/Resumes/autoCV (3).pdf',\n",
    "    '/workspaces/SIMS-Project/Resume_Scrapper/Resumes/resume_2 (1).pdf',\n",
    "    '/workspaces/SIMS-Project/Resume_Scrapper/Resumes/Resume.pdf',\n",
    "    '/workspaces/SIMS-Project/Resume_Scrapper/Resumes/Resume_2.pdf'\n",
    "]\n",
    "\n",
    "# Calculate scores for all resumes\n",
    "resume_scores = []\n",
    "for resume_file in resume_files:\n",
    "    try:\n",
    "        score = process_resume(resume_file)\n",
    "        resume_scores.append((resume_file, score))  # Store as (resume_file, score) tuple\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {resume_file}: {e}\")\n",
    "\n",
    "# Sort resumes by score (highest to lowest)\n",
    "resume_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print ranked resumes\n",
    "print(\"Ranked Resumes (Highest to Lowest):\")\n",
    "for rank, (resume_file, score) in enumerate(resume_scores, start=1):\n",
    "    print(f\"Rank {rank}: {resume_file[24:]} (Score: {score})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Resume_Scrapper/Resumes/autoCV (3).pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 125\u001b[0m\n\u001b[1;32m    123\u001b[0m resume_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m resume_file \u001b[38;5;129;01min\u001b[39;00m resume_files:\n\u001b[0;32m--> 125\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     resume_scores\u001b[38;5;241m.\u001b[39mappend((resume_file, score))  \u001b[38;5;66;03m# Store as (resume_file, score) tuple\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Sort resumes by score (highest to lowest)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 89\u001b[0m, in \u001b[0;36mprocess_resume\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_resume\u001b[39m(pdf_path):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Extract text from PDF\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     resume_text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Parse resume sections\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     resume_sections \u001b[38;5;241m=\u001b[39m parse_resume_sections(resume_text)\n",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m, in \u001b[0;36mextract_text_from_pdf\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text_from_pdf\u001b[39m(pdf_path):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpdfplumber\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pdf:\n\u001b[1;32m      7\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf\u001b[38;5;241m.\u001b[39mpages:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pdfplumber/pdf.py:95\u001b[0m, in \u001b[0;36mPDF.open\u001b[0;34m(cls, path_or_fp, pages, laparams, password, strict_metadata, unicode_norm, repair, gs_path, repair_setting, raise_unicode_errors)\u001b[0m\n\u001b[1;32m     93\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_fp, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath)):\n\u001b[0;32m---> 95\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_or_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     stream_is_external \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(path_or_fp)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Resume_Scrapper/Resumes/autoCV (3).pdf'"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def parse_resume_sections(resume_text):\n",
    "    sections = {\n",
    "        'Summary': '',\n",
    "        'Skills': '',\n",
    "        'Education': '',\n",
    "        'Work Experience': '',\n",
    "        'Projects': ''\n",
    "    }\n",
    "    \n",
    "    # Improved regex to handle section extraction\n",
    "    summary_match = re.search(r'Summary(.*?)(Skills|Education|Work Experience|Projects)', resume_text, re.DOTALL)\n",
    "    if summary_match:\n",
    "        sections['Summary'] = summary_match.group(1).strip()\n",
    "    \n",
    "    skills_match = re.search(r'Skills(.*?)(Education|Work Experience|Projects)', resume_text, re.DOTALL)\n",
    "    if skills_match:\n",
    "        sections['Skills'] = skills_match.group(1).strip()\n",
    "    \n",
    "    education_match = re.search(r'Education(.*?)(Work Experience|Projects)', resume_text, re.DOTALL)\n",
    "    if education_match:\n",
    "        sections['Education'] = education_match.group(1).strip()\n",
    "    \n",
    "    experience_match = re.search(r'Work Experience(.*?)Projects', resume_text, re.DOTALL)\n",
    "    if experience_match:\n",
    "        sections['Work Experience'] = experience_match.group(1).strip()\n",
    "    \n",
    "    projects_match = re.search(r'Projects(.*?)$', resume_text, re.DOTALL)\n",
    "    if projects_match:\n",
    "        sections['Projects'] = projects_match.group(1).strip()\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def save_sections_to_files(sections):\n",
    "    for section_name, section_content in sections.items():\n",
    "        with open(f'{section_name.lower().replace(\" \", \"_\")}.txt', 'w') as file:\n",
    "            file.write(section_content)\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def compare_and_extract_metrics(section_file, dataset, column_name, metric_column):\n",
    "    with open(section_file, 'r') as file:\n",
    "        section_content = file.read()\n",
    "    \n",
    "    # Find matches where the column_name appears in the section content\n",
    "    matches = dataset[dataset[column_name].apply(lambda x: str(x).lower() in section_content.lower())]\n",
    "    return matches[[metric_column]]\n",
    "\n",
    "def calculate_resume_score(company_ranks, skills_scores, university_rankings, has_work_experience):\n",
    "    # Use raw scores instead of normalized scores\n",
    "    company_score = company_ranks['Rank'].min() if not company_ranks.empty else 0\n",
    "    skills_score = skills_scores['Score'].sum() if not skills_scores.empty else 0\n",
    "    university_score = university_rankings['ranking'].min() if not university_rankings.empty else 0\n",
    "    \n",
    "    # Assign weights\n",
    "    company_weight = 2.0  # Adjust as needed\n",
    "    skills_weight = 1.5   # Skills might be more important\n",
    "    university_weight = 1.0  # Universities might be less important\n",
    "    \n",
    "    # Calculate weighted total score\n",
    "    total_score = (\n",
    "        (company_score * company_weight) +\n",
    "        (skills_score * skills_weight) +\n",
    "        (university_score * university_weight)\n",
    "    )\n",
    "    \n",
    "    # Apply penalty for resumes without work experience\n",
    "    if not has_work_experience:\n",
    "        total_score *= 0.8  # Reduce score by 20% if no work experience\n",
    "    \n",
    "    # Scale the final score to a range of 0 to 100\n",
    "    scaled_score = (total_score / 10)  # Adjust the divisor based on expected score range\n",
    "    return round(scaled_score, 2)\n",
    "\n",
    "# Function to process a resume and return its score\n",
    "def process_resume(pdf_path):\n",
    "    # Extract text from PDF\n",
    "    resume_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Parse resume sections\n",
    "    resume_sections = parse_resume_sections(resume_text)\n",
    "    \n",
    "    # Save sections to files\n",
    "    save_sections_to_files(resume_sections)\n",
    "    \n",
    "    # Load datasets\n",
    "    company_df = load_dataset('Resume_Scrapper/Datasets/Companies_Dataset.csv')\n",
    "    skills_df = load_dataset('Resume_Scrapper/Datasets/Skills_Dataset.csv')\n",
    "    universities_df = load_dataset('Resume_Scrapper/Datasets/Universities_Dataset.csv')\n",
    "    \n",
    "    # Compare and extract metrics\n",
    "    company_ranks = compare_and_extract_metrics('work_experience.txt', company_df, 'Name', 'Rank')\n",
    "    skills_scores = compare_and_extract_metrics('skills.txt', skills_df, 'Skill', 'Score')\n",
    "    university_rankings = compare_and_extract_metrics('education.txt', universities_df, 'University', 'ranking')\n",
    "    \n",
    "    # Check if work experience exists\n",
    "    has_work_experience = bool(resume_sections['Work Experience'])\n",
    "    \n",
    "    # Calculate resume score\n",
    "    resume_score = calculate_resume_score(company_ranks, skills_scores, university_rankings, has_work_experience)\n",
    "    \n",
    "    return resume_score\n",
    "\n",
    "# List of resume files\n",
    "resume_files = [\n",
    "    'Resume_Scrapper/Resumes/autoCV (3).pdf',\n",
    "    'Resume_Scrapper/Resumes/resume_2 (1).pdf',\n",
    "    'Resume_Scrapper/Resumes/Resume.pdf'\n",
    "]\n",
    "\n",
    "# Calculate scores for all resumes\n",
    "resume_scores = []\n",
    "for resume_file in resume_files:\n",
    "    score = process_resume(resume_file)\n",
    "    resume_scores.append((resume_file, score))  # Store as (resume_file, score) tuple\n",
    "\n",
    "# Sort resumes by score (highest to lowest)\n",
    "resume_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print ranked resumes\n",
    "print(\"Ranked Resumes (Highest to Lowest):\")\n",
    "for rank, (resume_file, score) in enumerate(resume_scores, start=1):\n",
    "    print(f\"Rank {rank}: {resume_file[24:]} (Score: {score})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resume_score_calc_1(resume_text):\n",
    "    # Split the resume text into lines\n",
    "    resume_lines = resume_text.split(\"\\n\")\n",
    "    \n",
    "    # Split each line into words and store them in a list\n",
    "    final_text = []\n",
    "    for line in resume_lines:\n",
    "        words = line.split(\" \")\n",
    "        final_text.extend(word.lower() for word in words)\n",
    "\n",
    "    # Track the character position in the original resume_text\n",
    "    char_idx = 0\n",
    "\n",
    "    # Iterate through the words to find keywords\n",
    "    for word in final_text:\n",
    "        for keyword in keywords:\n",
    "            if word == keyword.lower():  # Case-insensitive comparison\n",
    "                # Find the position of the keyword in the original resume_text\n",
    "                keyword_pos = resume_text.lower().find(keyword.lower(), char_idx)\n",
    "                if keyword_pos != -1:  # If the keyword is found\n",
    "                    # Write the text AFTER the keyword to a file\n",
    "                    with open(f'Resume_Scrapper/Downloaded/resume_text/{keyword}.txt', \"w\", encoding='utf-8') as f:\n",
    "                        f.write(resume_text[keyword_pos + len(keyword):].strip())\n",
    "                    print(f\"Downloaded: {keyword}.txt\")\n",
    "        # Update char_idx to the end of the current word\n",
    "        char_idx += len(word) + 1  # +1 for the space after the word\n",
    "\n",
    "def resume(resume_text):\n",
    "    count = 0\n",
    "    for keyword in keywords:\n",
    "        if keyword+\"\\n\" in resume_text:\n",
    "            print(resume_text[:resume_text.index(keyword + \"\\n\")])\n",
    "            count += 1\n",
    "\n",
    "def Resume_score_calc(resume_text):\n",
    "    # Split the resume text into lines\n",
    "    resume_lines = resume_text.split(\"\\n\")\n",
    "    \n",
    "    # Split each line into words and store them in a list\n",
    "    final_text = []\n",
    "    for line in resume_lines:\n",
    "        words = line.split(\" \")\n",
    "        final_text.extend(word.lower() for word in words)\n",
    "\n",
    "    # Track the character position in the original resume_text\n",
    "    char_idx = 0\n",
    "\n",
    "    # Dictionary to track if a keyword has been processed\n",
    "    keywords_processed = {keyword: False for keyword in keywords}\n",
    "\n",
    "    # Variable to store the previous keyword\n",
    "    prev_keyword = \"Basic_Info\"\n",
    "\n",
    "    # Iterate through the words to find keywords\n",
    "    for word in final_text:\n",
    "        for keyword in keywords:\n",
    "            if not keywords_processed[keyword] and word == keyword.lower():  # Case-insensitive comparison\n",
    "                # Write the text up to the current character position to a file\n",
    "                with open(f'Resume_Scrapper/Downloaded/resume_text/{prev_keyword}.txt', \"w\", encoding='utf-8') as f:\n",
    "                    f.write(resume_text[:char_idx].strip())\n",
    "                print(f\"Downloaded: {prev_keyword}.txt\")\n",
    "                \n",
    "                # Mark the keyword as processed\n",
    "                keywords_processed[keyword] = True\n",
    "                \n",
    "                # Reset resume_text and char_idx after writing\n",
    "                resume_text = resume_text[char_idx:]\n",
    "                char_idx = 0\n",
    "                prev_keyword = keyword\n",
    "                break  # Stop after the first keyword is found\n",
    "        # Update char_idx to the end of the current word\n",
    "        char_idx += len(word) + 1  # +1 for the space after the word\n",
    "\n",
    "    # Write the remaining text (after the last keyword) to the file for the last keyword\n",
    "    with open(f'Resume_Scrapper/Downloaded/resume_text/{prev_keyword}.txt', \"w\", encoding='utf-8') as f:\n",
    "        f.write(resume_text.strip())\n",
    "    print(f\"Downloaded: {prev_keyword}.txt\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "def Resume_score_calc_new(resume_text):\n",
    "    # Define keywords in the order they should appear\n",
    "    keywords_final = []\n",
    "    resume_lines = resume_text.split(\"\\n\")\n",
    "    \n",
    "    # Split each line into words and store them in a list\n",
    "    final_text = []\n",
    "    for line in resume_lines:\n",
    "        words = line.split(\" \")\n",
    "        final_text.extend(word.lower() for word in words)\n",
    "\n",
    "    # Track the character position in the original resume_text\n",
    "    char_idx = 0\n",
    "\n",
    "    # Dictionary to track if a keyword has been processed\n",
    "    keywords_processed = {keyword: False for keyword in keywords}\n",
    "\n",
    "    # Variable to store the previous keyword\n",
    "    prev_keyword = \"Basic_Info\"\n",
    "\n",
    "    # Iterate through the words to find keywords\n",
    "    for word in final_text:\n",
    "        for keyword in keywords:\n",
    "            if not keywords_processed[keyword] and word == keyword.lower(): \n",
    "                keywords_final.append(keyword)\n",
    "    print(keywords_final)\n",
    "    \n",
    "    # Create directory if needed\n",
    "    os.makedirs(\"Resume_Scrapper/Downloaded/resume_text\", exist_ok=True)\n",
    "    \n",
    "    # Preprocess text for better matching\n",
    "    processed_text = \"\\n\" + resume_text.replace(\"\\r\", \"\") + \"\\n\"\n",
    "    \n",
    "    sections = {}\n",
    "    remaining_text = processed_text\n",
    "    prev_section = \"Basic_Info\"\n",
    "    \n",
    "    # Find sections in order\n",
    "    for keyword in keywords_final:\n",
    "        # Regex pattern for section header detection\n",
    "        pattern = re.compile(\n",
    "            rf'\\n\\s*{re.escape(keyword)}\\s*\\n+',\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        \n",
    "        match = pattern.search(remaining_text)\n",
    "        if match:\n",
    "            # Extract content between previous section and this section\n",
    "            content = remaining_text[:match.start()].strip()\n",
    "            sections[prev_section] = content\n",
    "        \n",
    "            # Update remaining text and current section\n",
    "            remaining_text = remaining_text[match.end():]\n",
    "            prev_section = keyword.upper()\n",
    "    \n",
    "    # Add remaining content to last section\n",
    "    sections[prev_section] = remaining_text.strip()\n",
    "    \n",
    "    # Save to files\n",
    "    for section, content in sections.items():\n",
    "        filename = f\"{section.replace(' ', '_')}.txt\"\n",
    "        with open(f'Resume_Scrapper/Downloaded/resume_text/{filename}', 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    print(f\"Successfully extracted sections: {list(sections.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resume_score_calc_new(resume_text):\n",
    "    # Define keywords in the order they should appear\n",
    "    keywords_final = []\n",
    "    resume_lines = resume_text.split(\"\\n\")\n",
    "    \n",
    "    # Split each line into words and store them in a list\n",
    "    final_text = []\n",
    "    for line in resume_lines:\n",
    "        words = line.split(\" \")\n",
    "        final_text.extend(word.lower() for word in words)\n",
    "\n",
    "    # Track the character position in the original resume_text\n",
    "    char_idx = 0\n",
    "\n",
    "    # Dictionary to track if a keyword has been processed\n",
    "    keywords_processed = {keyword: False for keyword in keywords}\n",
    "\n",
    "    # Variable to store the previous keyword\n",
    "    prev_keyword = \"Basic_Info\"\n",
    "\n",
    "    # Iterate through the words to find keywords\n",
    "    for word in final_text:\n",
    "        for keyword in keywords:\n",
    "            if not keywords_processed[keyword] and word == keyword.lower(): \n",
    "                keywords_final.append(keyword)\n",
    "    \n",
    "    # Create directory if needed\n",
    "    os.makedirs(\"Resume_Scrapper/Downloaded/resume_text\", exist_ok=True)\n",
    "    \n",
    "    # Preprocess text for better matching\n",
    "    processed_text = \"\\n\" + resume_text.replace(\"\\r\", \"\") + \"\\n\"\n",
    "    \n",
    "    sections = {}\n",
    "    remaining_text = processed_text\n",
    "    prev_section = \"Basic_Info\"\n",
    "    \n",
    "    # Find sections in order\n",
    "    for keyword in keywords_final:\n",
    "        # Regex pattern for section header detection\n",
    "        pattern = re.compile(\n",
    "            rf'\\n\\s*{re.escape(keyword)}\\s*\\n+',\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        \n",
    "        match = pattern.search(remaining_text)\n",
    "        if match:\n",
    "            # Extract content between previous section and this section\n",
    "            content = remaining_text[:match.start()].strip()\n",
    "            sections[prev_section] = content\n",
    "        \n",
    "            # Update remaining text and current section\n",
    "            remaining_text = remaining_text[match.end():]\n",
    "            prev_section = keyword.upper()\n",
    "    \n",
    "    # Add remaining content to last section\n",
    "    sections[prev_section] = remaining_text.strip()\n",
    "    \n",
    "    # Save to files\n",
    "    for section, content in sections.items():\n",
    "        filename = f\"{section.replace(' ', '_')}.txt\"\n",
    "        with open(f'Resume_Scrapper/Downloaded/resume_text/{filename}', 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    print(f\"Successfully extracted sections: {list(sections.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # company_score = 0\n",
    "    # for company, rank in company_ranking.items():\n",
    "    #     # print(company)\n",
    "    #     for comp in final_text:\n",
    "    #         if company.lower() == comp:\n",
    "    #             print(f\"found {company}\")\n",
    "    #             company_score += rank\n",
    "    #             break\n",
    "    #             # for company_tier, score_tier in company_tiers:\n",
    "    #             #     if rank > company_tier[0] and rank < company_tier[1]:\n",
    "    #             #         company_score += score_tier\n",
    "\n",
    "    # if \"nvidia\" in final_text:\n",
    "    #     print(\"Found\")\n",
    "\n",
    "    # print(company_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = resume_text.split(\"\\n\")\n",
    "final_text = []\n",
    "for line in resume_text:\n",
    "    line = line.split(\" \")\n",
    "    final_text.extend(word.lower() for word in line)\n",
    "\n",
    "# print(final_text)\n",
    "\n",
    "company_score = 0\n",
    "for company, rank in company_ranking.items():\n",
    "    # print(company)\n",
    "    for comp in final_text:\n",
    "        if company.lower() == comp:\n",
    "            print(f\"found {company}\")\n",
    "            company_score += rank\n",
    "            break\n",
    "            # for company_tier, score_tier in company_tiers:\n",
    "            #     if rank > company_tier[0] and rank < company_tier[1]:\n",
    "            #         company_score += score_tier\n",
    "\n",
    "if \"nvidia\" in final_text:\n",
    "    print(\"Found\")\n",
    "\n",
    "print(company_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
